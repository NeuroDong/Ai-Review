<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Paper Review Assistant (Frontend)</title>
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <div class="container">
    <header style="display:flex; align-items:center; justify-content:space-between;">
      <h1 style="margin:0;">Ai Review</h1>
      <a href="https://github.com/NeuroDong/Ai-Review" target="_blank" rel="noopener noreferrer" title="Open GitHub repository">
        <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub repository" style="width:36px; height:36px; display:block;" />
      </a>
    </header>

    <div class="card">
      <label>Choose mode</label>
      <div style="display:flex; gap:12px; align-items:center; margin-bottom:8px;">
        <label><input type="radio" name="mode" value="userKey" checked /> Use My API Key</label>
        <label><input type="radio" name="mode" value="hosted" /> Quick Try</label>
      </div>

      <div id="userKeyArea">
        <label>API Provider</label>
        <select id="providerSelect">
          <option value="openrouter">OpenRouter (default)</option>
          <option value="deepseek">Deepseek</option>
        </select>
        <label>API Key (never stored)</label>
        <input id="apiKey" type="password" placeholder="sk-..." />
      </div>

      <div id="hostedArea" style="display:none;">
        <label>Hosted API URL (POST endpoint)</label>
        <input id="hostedUrl" type="text" placeholder="https://your-hosted.example.com/api/review" />
        <div class="hint">This internal API (deepseek/deepseek-chat-v3.1:free) allows up to 50 free calls per day. If the daily limit has been reached, please try again tomorrow or use your own API.</div>
      </div>

  <label id="modelLabel">Model (e.g., deepseek/deepseek-chat-v3.1:free, click <a href="https://openrouter.ai/models">here</a> for more models)</label>
  <input id="model" type="text" placeholder="deepseek-chat" />

      <label>Upload PDF (max 10MB)</label>
      <input id="pdfFile" type="file" accept="application/pdf" />
      <div id="fileHint" class="hint"></div>

      <label>Interface language / prompt selection</label>
      <select id="lang">
        <option value="en">English</option>
        <option value="zh">Chinese</option>
      </select>

      <label>Prompting mode</label>
      <div style="display:flex; gap:12px; align-items:center; margin-bottom:8px;">
        <label><input type="radio" name="promptMode" value="pure" checked /> Pure Prompt</label>
        <label><input type="radio" name="promptMode" value="fewshot" /> Prompt + Examples (Few-Shot)</label>
      </div>
      <div class="hint" id="fewshotHint" style="display:none; color:#475569;">
        The mode will feed the model some excellent review examples.
      </div>

      <button id="submitBtn">Send to model</button>
    </div>

    <div class="card">
      <h2>Model Response</h2>
      <div id="status" class="status">Waiting for user action</div>
      <div id="modelNotice" style="font-size:13px; color:#555; margin-top:6px; display:none;">Large model is processing<span id="modelDots">.</span></div>
      <pre id="response" class="response"></pre>
      <div class="response-actions">
        <button id="exportPdfBtn" disabled>Export response as Markdown file</button>
      </div>
    </div>

    <footer>
      <p>Note: API Key and PDF are processed only in the browser; the page will not store your API Key on any server or disk.</p>
    </footer>
  </div>

  <!-- pdf.js CDN -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.8.162/pdf.min.js"></script>
  <!-- html2pdf removed - exporting as Markdown now -->
  
  <!-- Inline fallback prompts/examples for file:// usage (no server). The app will prefer Prompts/*.txt and Examples/*.md via fetch; when blocked, it reads these. -->
  <script id="prompt-en-pure" type="text/markdown">
[System Role]
You are an experienced reviewer for top-tier ML/AI venues (AAAI/NeurIPS/ICLR style).
Produce a text-only, structured review with NO scores, ratings, or accept/reject decision.

[Critical Constraints]
1) Use EXACTLY these section headings in this order (no extras, no omissions):
   - Synopsis of the paper
   - Summary of Review
   - Strengths
   - Weaknesses
   - Suggestions for Improvement
   - References
2) Do NOT output any scores, ratings, or accept/reject verdict.
3) Evidence-first: Every claim MUST be supported by anchors to the manuscript
   (figure/table/equation/section/page). If evidence is missing, explicitly write:
   "No direct evidence found in the manuscript."
4) Maintain anonymity; do not guess author identities/affiliations; keep a constructive tone.
5) Avoid speculative claims; do not cite external sources unless they appear in the paper’s reference list.

[Input]
- Full anonymous manuscript (plain text or OCR output).

[Output Template]
Write the review using the six headings—exactly these and only these:

1) Synopsis of the paper
   - Concisely and neutrally restate the problem, method, core contributions, and main results (≤150 words).
   - Avoid subjective judgments or decision-like language.

2) Summary of Review
   - Provide 3–5 sentences summarizing your overall view (key pros AND cons).
   - After each reason, add an evidence anchor (e.g., "see Table 2; Sec. 4.1; Eq. (5)").
   - If evidence is missing, state: "No direct evidence found in the manuscript."

3) Strengths
   - Generate AS MANY items as the manuscript supports (≥3 encouraged; more is better).
   - Use UNNUMBERED bullet items with concise BOLDED titles (no numbering).
   - For each item, include sub-point examples (≥3 encouraged; more is better) that belong to the item.
   - Each sub-point example should include evidence (Figure/Table/Section/Page references supporting this strength) and why it matters (novelty/technical soundness/experimental rigor/clarity/impact).
   - Coverage suggestions (if information allows): problem setting/assumptions; relation to prior work; method limitations; experimental design/statistical significance; generalization/fairness/robustness; reproducibility/resource consumption; ethics/social impact; writing clarity; etc.

4) Weaknesses
   - Generate AS MANY items as the manuscript supports (≥3 encouraged; more is better).
   - Include one item that evaluates the correctness, clarity, or consistency of mathematical formulations (e.g., equations, notation, derivations).
   - Use UNNUMBERED bullet items with concise BOLDED titles (no numbering).
   - For each item, include sub-point examples (≥3 encouraged; more is better) that belong to the item.
   - Each sub-point example should include evidence (Figure/Table/Section/Page references supporting this strength) and why it matters (novelty/technical soundness/experimental rigor/clarity/impact).
   - Coverage suggestions (if information allows): problem setting/assumptions; relation to prior work; method limitations; experimental design/statistical significance; generalization/fairness/robustness; reproducibility/resource consumption; ethics/social impact; writing clarity; etc.

5) Suggestions for Improvement
   - Provide concrete, actionable, and verifiable recommendations; the number of recommendations should be the same as the number of Weaknesses, and they should correspond one to one.
   - Use UNNUMBERED bullet items with concise BOLDED titles (no numbering).
   - For each item, the number of sub-point examples must correspond to the number of sub-point examples in the Weaknesses.

6) References
   - List ONLY items that you explicitly cite within this review AND that appear in the manuscript’s reference list.
   - Use the following format for citations in the comments above:  [Author et al., Year].
   - If nothing is cited or the manuscript’s reference list is unavailable, write "None".

[Style & Length]
- Tone: objective, polite, and constructive.
- Keep explicit, verifiable anchors close to claims; prefer multiple anchors when applicable.
- Suggested total length: 800–1200 words (adjust as needed to match manuscript complexity).
  </script>

  <script id="prompt-en-fewshot" type="text/markdown">
Please refer to the contents of review_in_Resnet.md and review_in_Verified.md, and combine with the following requirements to generate review comments for the article.

[System Role]
You are an experienced reviewer for top-tier ML/AI venues (AAAI/NeurIPS/ICLR style).
Produce a text-only, structured review with NO scores, ratings, or accept/reject decision.

[Critical Constraints]
1) Use EXACTLY these section headings in this order (no extras, no omissions):
   - Synopsis of the paper
   - Summary of Review
   - Strengths
   - Weaknesses
   - Suggestions for Improvement
   - References
2) Do NOT output any scores, ratings, or accept/reject verdict.
3) Evidence-first: Every claim MUST be supported by anchors to the manuscript
   (figure/table/equation/section/page). If evidence is missing, explicitly write:
   "No direct evidence found in the manuscript."
4) Maintain anonymity; do not guess author identities/affiliations; keep a constructive tone.
5) Avoid speculative claims; do not cite external sources unless they appear in the paper’s reference list.

[Input]
- Full anonymous manuscript (plain text or OCR output).

[Output Template]
Write the review using the six headings—exactly these and only these:

1) Synopsis of the paper
   - Concisely and neutrally restate the problem, method, core contributions, and main results (≤150 words).
   - Avoid subjective judgments or decision-like language.

2) Summary of Review
   - Provide 3–5 sentences summarizing your overall view (key pros AND cons).
   - After each reason, add an evidence anchor (e.g., "see Table 2; Sec. 4.1; Eq. (5)").
   - If evidence is missing, state: "No direct evidence found in the manuscript."

3) Strengths
   - Generate AS MANY items as the manuscript supports (≥3 encouraged; more is better).
   - Use UNNUMBERED bullet items with concise BOLDED titles (no numbering).
   - For each item, include sub-point examples (≥3 encouraged; more is better) that belong to the item.
   - Each sub-point example should include evidence (Figure/Table/Section/Page references supporting this strength) and why it matters (novelty/technical soundness/experimental rigor/clarity/impact).
   - Coverage suggestions (if information allows): problem setting/assumptions; relation to prior work; method limitations; experimental design/statistical significance; generalization/fairness/robustness; reproducibility/resource consumption; ethics/social impact; writing clarity; etc.

4) Weaknesses
   - Generate AS MANY items as the manuscript supports (≥3 encouraged; more is better).
   - Include one item that evaluates the correctness, clarity, or consistency of mathematical formulations (e.g., equations, notation, derivations).
   - Use UNNUMBERED bullet items with concise BOLDED titles (no numbering).
   - For each item, include sub-point examples (≥3 encouraged; more is better) that belong to the item.
   - Each sub-point example should include evidence (Figure/Table/Section/Page references supporting this strength) and why it matters (novelty/technical soundness/experimental rigor/clarity/impact).
   - Coverage suggestions (if information allows): problem setting/assumptions; relation to prior work; method limitations; experimental design/statistical significance; generalization/fairness/robustness; reproducibility/resource consumption; ethics/social impact; writing clarity; etc.

5) Suggestions for Improvement
   - Provide concrete, actionable, and verifiable recommendations; the number of recommendations should be the same as the number of Weaknesses, and they should correspond one to one.
   - Use UNNUMBERED bullet items with concise BOLDED titles (no numbering).
   - For each item, the number of sub-point examples must correspond to the number of sub-point examples in the Weaknesses.

6) References
   - List ONLY items that you explicitly cite within this review AND that appear in the manuscript’s reference list.

[Style & Length]
- Tone: objective, polite, and constructive.
- Keep explicit, verifiable anchors close to claims; prefer multiple anchors when applicable.
- Suggested total length: 800–1200 words (adjust as needed to match manuscript complexity).
  </script>

  <script id="prompt-zh-pure" type="text/markdown">
[系统角色]
您是一名经验丰富的顶级机器学习/人工智能会议（AAAI/NeurIPS/ICLR 风格）的审稿人。
请生成仅包含文本、结构化的审稿意见，且不得包含任何分数、评级或接收/拒绝决定。

[关键约束]
1. 必须严格使用以下章节标题，且顺序完全一致（不得增删）：
   - Synopsis of the paper
   - Summary of Review
   - Strengths
   - Weaknesses
   - Suggestions for Improvement
   - References

2. 不得输出任何分数、评级或接收/拒绝结论。

3. 基于证据优先原则：每个观点必须有来自稿件的证据支撑
   （如：figure/table/equation/section/page）。
   如果稿件中缺少证据，请明确写出：
   “No direct evidence found in the manuscript.”
  - 证据锚点应尽可能具体到图号/表号/公式编号/章节或页码，推荐书写示例：
    “see Table 2; Sec. 4.1; Eq. (5); Fig. 3; p. 12”。

4. 保持匿名性：不要猜测作者身份或单位；保持建设性语气。

5. 避免推测性陈述：不要引用稿件之外的外部资料，除非该资料已出现在论文参考文献中。

[输入]
- 完整的匿名稿件（纯文本或 OCR 输出）。

[输出模板]
请严格按照以下六个标题撰写审稿意见（仅这六个标题，不得增删）：

1) Synopsis of the paper
- 简明客观地重述论文的问题、方法、核心贡献和主要结果（≤150 字）。
- 避免主观判断或带有决定倾向的语言。

2) Summary of Review
- 用 3–5 句话总结整体看法（包括主要优点和缺点）。
- 每个理由后添加证据锚点（如：“see Table 2; Sec. 4.1; Eq. (5)”）。
- 如果缺少证据，请写：“No direct evidence found in the manuscript.”

3) Strengths
- 根据稿件支持，列出尽可能多的优点（建议 ≥3 条，越多越好）。
- 使用无编号的项目符号，每项需有简洁加粗标题（不编号）。
 - 覆盖面建议（如信息允许）：方法设计、理论分析、实验结果、消融/鲁棒性、复现性/开源、写作与结构、潜在影响等。
 - 每个优点下包含至少 4 个子点示例（建议 4–6 个），每个示例需包含：
  - 证据来源（Figure/Table/Section/Page）
  - 以及其重要性（如新颖性、技术可靠性、实验严谨性、清晰度、影响力）。
 - 每个子点应为独立句或独立行，避免将多个要点挤在同一句；尽可能加入与基线/消融的对比说明。

4) Weaknesses
- 根据稿件支持，列出尽可能多的缺点（建议 ≥3 条，越多越好）。
- 包括一项评估数学公式（例如方程式、符号、推导）的正确性、清晰度或一致性的弱点。
- 使用无编号的项目符号，每项需有简洁加粗标题（不编号）。
 - 覆盖面建议（如信息允许）：问题设定/假设、与相关工作关系、方法局限、实验设计/统计显著性、泛化/公平/稳健、可复现性/资源消耗、伦理/社会影响、写作清晰度等。
 - 每个缺点下包含至少 4 个子点示例（建议 4–6 个），每个示例需包含：
  - 证据来源（Figure/Table/Section/Page）
  - 以及其重要性（如新颖性、技术可靠性、实验严谨性、清晰度、影响力）。
 - 针对“数学公式/符号/推导一致性”的条目，建议至少给出 4 个具体证据点（如式号、定义、推导步骤、记号复用处）。

5) Suggestions for Improvement
- 提供具体、可执行、可验证的改进建议；
- 建议数量应与 Weaknesses 数量一致，并一一对应。
- 使用无编号的项目符号，每项需有简洁加粗标题（不编号）。
- 每个建议下的子点数量应与对应 Weaknesses 的子点数量一致。
 - 每个子点尽量具体到：
   - 可执行检查项或操作步骤（例如：新增对照、统计检验、超参扫描范围、消融设置、误差条/置信区间报告方式）；
   - 可验证的预期指标或判据（例如：在 Table X 的某列提升 ≥y%，或置信区间重叠情况消失）；
   - 复现实验的必要信息（例如：随机种子、数据划分、训练时长/显存预算、代码/模型链接，如可用）。

6) References
- 仅列出在本审稿意见中明确引用且出现在稿件参考文献中的条目。
- 引用格式：[Author et al., Year]。
- 如果未引用或稿件无参考文献，请写：“None”。

[风格与长度]
- 语气：客观、礼貌、建设性。
- 紧贴证据锚点，优先多重证据。
 - 建议总长度：建议 1200–1800 字；最低不少于 1000 字（复杂稿件可适当超出）。
 - 为保证信息密度与可读性：
   - Summary of Review 中每个理由后务必附至少 1 个证据锚点；
   - Strengths 与 Weaknesses 的每个条目下应包含 4–6 个有证据的子点；
   - 避免空泛表述，尽量引用稿件中的变量/符号/设定名称与编号以提高可核查性。
  </script>

  <script id="prompt-zh-fewshot" type="text/markdown">
请参考review_in_Resnet.md和review_in_Verified.md的审稿内容，并结合以下要求，为文章生成评审意见。

[系统角色]
您是一名经验丰富的顶级机器学习/人工智能会议（AAAI/NeurIPS/ICLR 风格）的审稿人。
请生成仅包含文本、结构化的审稿意见，且不得包含任何分数、评级或接收/拒绝决定。

[关键约束]
1. 必须严格使用以下章节标题，且顺序完全一致（不得增删）：
   - Synopsis of the paper
   - Summary of Review
   - Strengths
   - Weaknesses
   - Suggestions for Improvement
   - References

2. 不得输出任何分数、评级或接收/拒绝结论。

3. 基于证据优先原则：每个观点必须有来自稿件的证据支撑
   （如：figure/table/equation/section/page）。
   如果稿件中缺少证据，请明确写出：
   “No direct evidence found in the manuscript.”
  - 证据锚点应尽可能具体到图号/表号/公式编号/章节或页码，推荐书写示例：
    “see Table 2; Sec. 4.1; Eq. (5); Fig. 3; p. 12”。

4. 保持匿名性：不要猜测作者身份或单位；保持建设性语气。

5. 避免推测性陈述：不要引用稿件之外的外部资料，除非该资料已出现在论文参考文献中。

[输入]
- 完整的匿名稿件（纯文本或 OCR 输出）。

[输出模板]
请严格按照以下六个标题撰写审稿意见（仅这六个标题，不得增删）：

1) Synopsis of the paper
- 简明客观地重述论文的问题、方法、核心贡献和主要结果（≤150 字）。
- 避免主观判断或带有决定倾向的语言。

2) Summary of Review
- 用 3–5 句话总结整体看法（包括主要优点和缺点）。
- 每个理由后添加证据锚点（如：“see Table 2; Sec. 4.1; Eq. (5)”）。
- 如果缺少证据，请写：“No direct evidence found in the manuscript.”

3) Strengths
- 根据稿件支持，列出尽可能多的优点（建议 ≥3 条，越多越好）。
- 使用无编号的项目符号，每项需有简洁加粗标题（不编号）。
 - 覆盖面建议（如信息允许）：方法设计、理论分析、实验结果、消融/鲁棒性、复现性/开源、写作与结构、潜在影响等。
 - 每个优点下包含至少 4 个子点示例（建议 4–6 个），每个示例需包含：
  - 证据来源（Figure/Table/Section/Page）
  - 以及其重要性（如新颖性、技术可靠性、实验严谨性、清晰度、影响力）。
 - 每个子点应为独立句或独立行，避免将多个要点挤在同一句；尽可能加入与基线/消融的对比说明。

4) Weaknesses
- 根据稿件支持，列出尽可能多的缺点（建议 ≥3 条，越多越好）。
- 包括一项评估数学公式（例如方程式、符号、推导）的正确性、清晰度或一致性的弱点。
- 使用无编号的项目符号，每项需有简洁加粗标题（不编号）。
 - 覆盖面建议（如信息允许）：问题设定/假设、与相关工作关系、方法局限、实验设计/统计显著性、泛化/公平/稳健、可复现性/资源消耗、伦理/社会影响、写作清晰度等。
 - 每个缺点下包含至少 4 个子点示例（建议 4–6 个），每个示例需包含：
  - 证据来源（Figure/Table/Section/Page）
  - 以及其重要性（如新颖性、技术可靠性、实验严谨性、清晰度、影响力）。
 - 针对“数学公式/符号/推导一致性”的条目，建议至少给出 4 个具体证据点（如式号、定义、推导步骤、记号复用处）。

5) Suggestions for Improvement
- 提供具体、可执行、可验证的改进建议；
- 建议数量应与 Weaknesses 数量一致，并一一对应。
- 使用无编号的项目符号，每项需有简洁加粗标题（不编号）。
- 每个建议下的子点数量应与对应 Weaknesses 的子点数量一致。
 - 每个子点尽量具体到：
   - 可执行检查项或操作步骤（例如：新增对照、统计检验、超参扫描范围、消融设置、误差条/置信区间报告方式）；
   - 可验证的预期指标或判据（例如：在 Table X 的某列提升 ≥y%，或置信区间重叠情况消失）；
   - 复现实验的必要信息（例如：随机种子、数据划分、训练时长/显存预算、代码/模型链接，如可用）。

6) References
- 仅列出在本审稿意见中明确引用且出现在稿件参考文献中的条目。
- 引用格式：[Author et al., Year]。
- 如果未引用或稿件无参考文献，请写：“None”。

[风格与长度]
- 语气：客观、礼貌、建设性。
- 紧贴证据锚点，优先多重证据。
 - 建议总长度：建议 1200–1800 字；最低不少于 1000 字（复杂稿件可适当超出）。
 - 为保证信息密度与可读性：
   - Summary of Review 中每个理由后务必附至少 1 个证据锚点；
   - Strengths 与 Weaknesses 的每个条目下应包含 4–6 个有证据的子点；
   - 避免空泛表述，尽量引用稿件中的变量/符号/设定名称与编号以提高可核查性。
  </script>

  <script id="example-resnet" type="text/markdown">
# Synopsis of the paper
- The manuscript introduces a residual learning framework for training very deep convolutional neural networks by reformulating target mappings as residual functions added via identity shortcut connections (Eq. (1); Fig. 2; Sec. 3.1–3.2; p.2–3). It presents architectures up to 152 layers on ImageNet and over 1000 layers on CIFAR-10, with bottleneck blocks to control complexity (Fig. 3–5; Sec. 3.3; Table 1; p.3–6). Empirical evaluations show improved optimization behavior and accuracy compared to plain networks, with state-of-the-art results on ImageNet classification (Tables 3–5; Sec. 4.1; p.5–6) and substantial gains in detection/localization tasks (Sec. 4.3; Tables 7–14; p.8–12).

# Summary of Review
- The paper addresses the degradation problem in deep networks with a simple, well-motivated residual formulation and rigorous, large-scale experiments (Fig. 1; Sec. 1; Sec. 3.1–3.2; Sec. 4.1; p.1–6). 
- Empirical evidence is broad and convincing across ImageNet, CIFAR-10, COCO, and VOC, with clear architecture descriptions and training protocols (Fig. 3–6; Table 1; Sec. 3.4; Sec. 4.1–4.3; p.3–12).
- However, theoretical justification for why residual learning improves optimization is largely intuitive; formal analysis is limited and deferred (Sec. 3.1; statement referencing future work; p.3). No direct evidence found in the manuscript.
- Some implementation details and ablation analyses (e.g., role of BN, shortcut type choices, initialization sensitivity) could be expanded for reproducibility and to isolate causal factors (Sec. 3.4; Table 3 options A/B/C; Fig. 4; p.4–6).

# Strengths
- **Clear identification of the degradation problem and motivation**
  - The manuscript documents increased training error with depth in plain nets on CIFAR-10 and ImageNet, motivating the need for better formulations (Fig. 1; Fig. 4 left; Sec. 1; p.1,4). This matters for technical soundness, grounding the contribution in observed failure modes.
  - The constructed-solution argument (identity layers) highlights optimization gaps without overfitting explanations (Sec. 1; p.1–2), improving conceptual clarity.
  - The residual formulation F(x)+x is presented succinctly with minimal changes to standard CNNs (Eq. (1); Fig. 2; Sec. 3.1–3.2; p.2–3), aiding broad impact and usability.

- **Simple, parameter-efficient architectural design with strong empirical gains**
  - Identity shortcuts add no parameters; projections are used only for dimension changes (Eq. (2); Fig. 3; Sec. 3.2–3.3; p.3–4), demonstrating efficiency—important for scalability.
  - Bottleneck blocks (1×1–3×3–1×1) enable depths 50/101/152 while keeping FLOPs below VGG-16/19 (Fig. 5; Table 1 FLOPs; Sec. 3.3; p.5–6), showing practical resource-conscious design.
  - Consistent validation gains as depth increases (Tables 3–4; Fig. 4 right; p.5–6) indicate robustness and impact.

- **Comprehensive large-scale experiments and cross-task generalization**
  - ImageNet classification results: single-model top-5 error 4.49% (ResNet-152) and ensemble 3.57% on test (Tables 4–5; Sec. 4.1; p.6), evidencing state-of-the-art performance.
  - CIFAR-10: successful optimization to 110 and 1202 layers, with training/testing curves and analysis of residual response magnitudes (Fig. 6–7; Table 6; Sec. 4.2; p.7–8), supporting claims of optimization ease and behavior.
  - Detection/localization: substantial mAP improvements on COCO/VOC and top-5 localization error reduction to 9.0% (Tables 9–14; Sec. 4.3; p.8–12), demonstrating transferability and real-world utility.

- **Clarity of network specifications and training protocol**
  - Architecture layouts and downsampling positions are detailed (Fig. 3; Table 1; Sec. 3.3; p.3–5), improving reproducibility.
  - Training settings (batch size, LR schedule, augmentation, BN placement) are documented (Sec. 3.4; p.4), providing practical guidance.

# Weaknesses
- **Limited theoretical grounding for optimization improvements**
  - The core hypothesis that residual functions are easier to optimize is argued qualitatively; no formal convergence or landscape analysis is provided (Sec. 3.1; p.3). This matters for technical soundness and generalization beyond reported regimes.
  - The manuscript conjectures exponentially low convergence rates for deep plain nets without empirical diagnostics beyond curves (Sec. 4.1; p.5). No direct evidence found in the manuscript.
  - The relationship to prior shortcut/gated architectures (e.g., highway networks) is descriptive; conditions under which identity shortcuts outperform gates are not theoretically characterized (Sec. 2; p.2–3).

- **Ablation coverage and factor isolation are incomplete**
  - The roles of batch normalization, initialization, and learning-rate warm-up are acknowledged but not isolated via controlled ablations (Sec. 3.4; Sec. 4.2 warm-up note; Fig. 6 middle; p.4,7). This impacts experimental rigor and reproducibility.
  - Shortcut variants A/B/C are compared, but quantitative analysis of where projection shortcuts help most (e.g., layer-wise) is limited (Table 3; Sec. “Identity vs. Projection Shortcuts”; p.6).
  - The claim that identity shortcuts are sufficient for addressing degradation lacks targeted tests removing BN or altering normalization to confirm necessity/sufficiency (Fig. 4; Sec. 3.2; p.4–6).

- **Notation and consistency issues in mathematical formulations**
  - The residual block uses y = F(x;{Wi}) + x (Eq. (1); p.3), but activation placement (“second nonlinearity after the addition”) could be ambiguous across implementations; clearer operator order diagrams would help (Fig. 2; Sec. 3.2; p.3).
  - The explanation of when W_s projections are required versus identity with zero-padding could be clarified with explicit dimensional constraints and stride interactions (Eq. (2); Fig. 3; p.3–4).
  - Discussion of response magnitudes (std after BN) would benefit from explicit definitions and aggregation procedures (Fig. 7 captions; Sec. 4.2; p.8), as current text is concise but not fully formal.

- **Reproducibility and resource reporting gaps**
  - While FLOPs are listed, memory footprints, training wall-clock, and hardware specs per model/depth are not reported (Table 1; Sec. 3.4; p.4–6), limiting practical adoption planning.
  - Detection/localization pipelines include multiple improvements; some choices (e.g., fixing BN statistics during fine-tuning) could use more justification and sensitivity checks (Appendix A; p.10). 
  - For the 1202-layer CIFAR-10 model, overfitting is hypothesized without regularization studies (Table 6; Fig. 6 right; Sec. 4.2; p.7–8).

# Suggestions for Improvement
- **Strengthen theoretical framing of residual optimization advantages**
  - Provide a formal analysis or empirical diagnostics (e.g., loss landscape curvature, gradient norms, or Hessian spectra) comparing plain vs. residual blocks across depths and datasets (Sec. 3.1; Fig. 4–6; p.3–7), clarifying mechanisms behind improved convergence.
  - Characterize when identity shortcuts are preferable to gated/projection alternatives with assumptions on activation/normalization, possibly via controlled synthetic studies (Sec. 2–3; p.2–4).
  - Include a discussion connecting residual learning to known preconditioning interpretations, with measurable quantities (No direct evidence found in the manuscript).

- **Expand ablations to isolate critical components**
  - Conduct BN-off/BN-on comparisons, different initialization schemes, and learning-rate warm-up/no warm-up across depths to quantify contributions (Sec. 3.4; Sec. 4.2; p.4,7).
  - Provide layer-wise analyses for options A/B/C showing where projection shortcuts materially change gradients or activations (Table 3; p.6).
  - Test necessity/sufficiency: e.g., plain nets with BN and identity shortcuts selectively removed to validate the attribution of gains to residual formulation (Fig. 4; p.4–5).

- **Clarify mathematical and notation aspects of blocks**
  - Add explicit diagrams or equations specifying activation order (pre-activation vs. post-activation variants), with consistency across figures and text (Fig. 2; Sec. 3.2; p.3).
  - Detail dimensionality constraints and stride behaviors requiring W_s, including zero-padding exact rules and potential artifacts (Eq. (2); Fig. 3; p.3–4).
  - Formalize residual response statistics: define computation points (post-BN/pre-ReLU), aggregation across layers/batches, and report summary tables with confidence intervals (Fig. 7; Sec. 4.2; p.8).

- **Enhance reproducibility and practical reporting**
  - Add memory usage, training time, and hardware details per model; include scalability guidance for different batch sizes and GPUs (Table 1; Sec. 3.4; p.4–6).
  - In detection/localization, justify fixing BN during fine-tuning with ablations (on/off) and discuss effects on mAP/latency (Appendix A; p.10–11).
  - For CIFAR-10 1202-layer model, run regularization ablations (dropout/maxout/weight decay settings) to substantiate overfitting claims and report best practices (Table 6; Fig. 6 right; p.7–8).

# References
- \[16\] Ioffe, S., & Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015 (appears in manuscript’s reference list; cited in Sec. 3.4; p.4).
- \[41\] Simonyan, K., & Zisserman, A. Very deep convolutional networks for large-scale image recognition. ICLR, 2015 (appears in manuscript’s reference list; used as comparisons; Tables 3–4; p.5–6).
- \[44\] Szegedy, C., et al. Going deeper with convolutions. CVPR, 2015 (appears in manuscript’s reference list; used as comparisons; Table 4; p.6).
- \[32\] Ren, S., He, K., Girshick, R., & Sun, J. Faster R-CNN: Towards real-time object detection with region proposal networks. NIPS, 2015 (appears in manuscript’s reference list; cited in Sec. 4.3; p.10–11).
  </script>

  <script id="example-verified" type="text/markdown">
# Synopsis of the paper
The manuscript studies probability calibration of predictive models and argues that widely used continuous-output recalibrators (e.g., Platt/temperature scaling) are less calibrated than reported because their true calibration error cannot be reliably measured with finite binning. It introduces the **scaling–binning calibrator**, which first fits a parametric scaling function and then bins its outputs to guarantee measurable calibration with improved sample complexity. The paper also analyzes calibration-error estimation and shows that a debiased estimator from meteorology yields tighter estimates than the common plugin estimator. Experiments on CIFAR-10 and ImageNet demonstrate lower (top-label and marginal) calibration error than histogram binning at comparable or fewer samples and more accurate estimation of ECE/CE. (Abstract; Sec. 3–5; Fig. 1–4, 7; Theorem 4.1; Theorems 5.3–5.4)

# Summary of Review
Overall, the paper offers a clear problem formulation and practical algorithmic contribution with rigorous analysis and extensive empirical validation. The core insight—that binning underestimates calibration error for continuous methods and that discretized outputs enable verifiable calibration—is well supported (Sec. 3; Prop. 3.3; Fig. 2, 5–6). At the same time, certain assumptions and choices (e.g., well-balanced binning, injectivity/consistency of scaling family) could be stated more prominently with clearer implications for practitioners (Sec. 4; Lemma 4.3; Assumptions 1–4). The experimental section is strong but could benefit from additional baselines (e.g., Dirichlet/beta calibration, vector scaling) and ablations on bin count selection and data splits (Sec. 4.3; App. E–G). The mathematical presentation is mostly correct, though a few places would gain from tightened notation (e.g., use of CE vs. ℓ_p-CE across statements, constants in bounds). (Sec. 3; App. B, D–F).

# Strengths
- **Evidence-driven critique of continuous recalibration**
  - Demonstrates that binned estimates of calibration error monotonically increase with more bins, indicating underestimation for continuous methods (Sec. 3; Fig. 2a–b, 5–6). This is impactful because it challenges prevailing evaluation practice and motivates verifiable alternatives. 
  - Provides constructive theory: Example 3.2 and Proposition 3.3 formally show how binning can hide true error and why coarse binning yields optimistic assessments (Sec. 3; App. B). This enhances technical soundness. 
  - Clear separation of method vs. evaluation binning clarifies prior conflation (Sec. 3, 4.1). This improves clarity and reproducibility. 

- **Novel scaling–binning calibrator with provable sample complexity**
  - Algorithm combining parametric scaling (Step 1) with uniform-mass binning and discretization (Steps 2–3) is well specified (Sec. 4.1; Fig. 1c; Steps 1–3). It offers a practical path to measurable calibration. 
  - Theorem 4.1 shows CE(^g_B)^2 ≤ 2·min_{g∈G} CE(g)^2 + ε^2 with n ≳ B log B + log(B)/ε^2 samples, establishing B+1/ε^2 scaling vs. histogram binning’s B/ε^2 (Sec. 4.2; Thm. 4.1). This is a strong theoretical contribution. 
  - Lemma 4.3 (well-balanced binning) and Lemma D.2 (empirical binning convergence) add rigor and explain why discretization does not overly harm sharpness (Prop. D.4) (Sec. 4.2; App. D). This addresses technical soundness and practical impact. 

- **Improved estimation of calibration error**
  - Defines plugin vs. debiased estimators and proves sample complexities: plugin O(B/ε²) vs. debiased O(√B/ε²) for estimating squared CE within a constant factor (Sec. 5; Thm. 5.3–5.4). This is novel and important for evaluation practice. 
  - Empirical verification on CIFAR-10/ImageNet shows lower mean-squared deviation from ground-truth estimates for the debiased estimator, especially when B is large or n is small (Sec. 5.1; Fig. 4, 12–16). This demonstrates experimental rigor. 
  - Extension to debiased ECE via Gaussian approximation provides a pragmatic improvement for ℓ₁ calibration metrics (App. G.1; Fig. 12–16). This broadens applicability. 

- **Comprehensive experiments and reproducibility**
  - Multiclass marginal and top-label calibration across CIFAR-10 and ImageNet, with bootstrap CIs, ablations on bins/samples, and synthetic validations (Sec. 4.3; Fig. 3, 7–11; App. E). This indicates solid empirical coverage. 
  - Open-source library and CodaLab/GitHub links for code/data (Page 10; “Reproducibility”). This supports replication. 

# Weaknesses
- **Assumption visibility and practitioner guidance**
  - Key regularity assumptions (finite parameters, injectivity, consistency, Lipschitz, twice differentiability) are relegated to App. D; clearer upfront guidance on when common calibrators (sigmoid, vector/Dirichlet/beta) satisfy these would aid users (App. D; Assumptions 1–4). This affects clarity and external validity. 
  - The “2–well-balanced” binning property is crucial for both Theorem 4.1 and estimator guarantees, yet practical procedures for ensuring/diagnosing it are brief (Sec. 4.2; Lemma 4.3). More diagnostics would improve robustness. 
  - Sensitivity of performance to bin count B and to merging of T1/T2/T3 in practice is noted but not fully quantified in the main paper (App. E). This limits prescriptive guidance. 

- **Scope of baselines and comparative positioning**
  - Baselines center on histogram binning vs. the proposed method; direct comparisons to **beta calibration** and **Dirichlet calibration** (NeurIPS’19) are only mentioned peripherally (App. E; Ref. [50], [25]) without empirical head-to-heads. This impacts claims of superiority across methods. 
  - Vector scaling / class-wise Platt variants and post-hoc multiclass calibrators beyond per-class binning are not systematically evaluated (Sec. 2.2; App. E). This affects completeness.
  - Evaluation primarily uses VGG16; diversity across architectures (e.g., ResNet, DenseNet) could reinforce generality (Sec. 4.3; App. E). No direct evidence found in the manuscript.

- **Mathematical presentation and notation consistency**
  - The switch between CE and ℓ_p-CE (Sec. 3 vs. App. B) can be confusing; explicit notation conventions early on would help (Sec. 3; App. B). This affects clarity.
  - Some bounds hide constants/log factors (“e” notation) without concrete ranges (Prop. D.4; App. D). Explicit constants would aid reproducibility.
  - Minor ambiguities in algorithmic description (e.g., separate datasets T1/T2/T3 vs. merged in practice—App. E) could be clarified within the main text.

- **Generalization beyond image classifiers**
  - Claims emphasize calibration in classification; while CE/ECE definitions extend to regression (e.g., Brier score discussion in Sec. 2.2), experiments focus on image multiclass tasks. Broader domains (NLP, tabular risk models) are noted in related work but not evaluated (Sec. 6). This limits demonstrated external validity.

# Suggestions for Improvement
- **Elevate assumptions & provide practitioner diagnostics**
  - Move Assumptions 1–4 (App. D) into the main text with concrete examples showing that common scaling families satisfy injectivity/regularity; add a brief checklist for users (Sec. 4). Mirror with a small table mapping families to assumptions. (Match the three sub-points above.)
  - Provide a practical test/heuristic for 2–well-balanced binning (e.g., empirical bin occupancy thresholds; QQ plots of bin mass) and report sensitivity when it is violated (Sec. 4.2). (Match the three sub-points above.)
  - Quantify sensitivity to B and to merging T1/T2/T3 via ablations in main text (not only App. E); include guidance for choosing B under sample constraints (Sec. 4.3). (Match the three sub-points above.)

- **Expand baselines & positioning**
  - Add empirical comparisons with **beta calibration** (App. E; Ref. [50]) and **Dirichlet calibration** (Ref. [25]) on CIFAR-10/ImageNet with identical splits; include vector scaling and multiclass post-hoc methods. (Match the three sub-points above.)
  - Evaluate on alternative architectures (e.g., ResNet-50, DenseNet-121) to demonstrate method robustness beyond VGG. (Match the three sub-points above.) No direct evidence found in the manuscript.
  - Summarize comparative takeaways in a positioning table (method class, measurability, sample complexity, CE/ECE results). (Match the three sub-points above.)

- **Tighten math and notation**
  - Unify CE vs. ℓ_p-CE notation and state global conventions at the start of Sec. 2–3; when switching metrics, restate definitions inline (Sec. 3; App. B). (Match the three sub-points above.)
  - Provide explicit constants (or ranges) hidden by \(\tilde{O}\)/e-notation in bounds (e.g., Prop. D.4) and annotate dependence on B, n, and bin-balance parameters; add a short “constants and logs” appendix table (App. D). (Match the three sub-points above.)
  - Clarify the algorithmic data split (T1/T2/T3) versus practical merging (App. E) within Sec. 4.1 and specify recommended splits for different n. (Match the three sub-points above.)

- **Broaden empirical scope**
  - Include at least one non-image task (e.g., tabular risk prediction or NLP classification) to demonstrate generality of scaling–binning and debiased estimation; report CE/ECE and MSE trade-offs (Sec. 6). (Match the three sub-points above.)

# References
- Guo et al., “On calibration of modern neural networks” (ICML 2017) — cited in Sec. 2.3, 3 (for temperature scaling baseline).
- Kull et al., “Beyond sigmoids: … beta calibration” (EJS 2017) — referenced in App. E for synthetic experiments’ scaling family.
- Kull et al., “Dirichlet calibration” (NeurIPS 2019) — referenced in References list and multiclass calibration context (Page 11; Ref. [25]).
  </script>

  <script src="config.js"></script>
  <script src="app.js"></script>
</body>
</html>